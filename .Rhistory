X.quanti <- splitmix(lovoo_data)$X.quanti %>% scale()
X.quali <- splitmix(lovoo_data)$X.quali
library(magrittr)
library(dplyr)
## Remove labels
lovoo_data = lovoo_data %>%
select(-c(counts_profileVisits, counts_kisses, conversion)) # remove if necessary
X.quanti <- splitmix(lovoo_data)$X.quanti %>% scale()
X.quali <- splitmix(lovoo_data)$X.quali
## Remove labels
lovoo_data = lovoo_data %>%
select(-c(counts_profileVisits, counts_kisses, conversion)) # remove if necessary
lovoo_data$highlighted = factor(lovoo_data$highlighted)
## Remove labels
lovoo_data = lovoo_data %>%
select(-c(counts_profileVisits, counts_kisses, conversion)) # remove if necessary
X.quanti <- splitmix(lovoo_data)$X.quanti %>% scale()
library(RCurl)
library(dplyr)
library(tidyverse)
library(tidyverse)
library(mosaic)
library(LICORS)
library(tidyverse)
library(usmap)
library(lubridate)
library(randomForest)
library(splines)
library(pdp)
# Power grid load every hour for 6 1/2 years
# throughout the 8 ERCOT regions of Texas
# units of grid load are megawatts.
# This represents peak instantaneous demand for power in that hour.
# source: scraped from the ERCOT website
load_data = read.csv("../data/ercot/load_data.csv")
# Power grid load every hour for 6 1/2 years
# throughout the 8 ERCOT regions of Texas
# units of grid load are megawatts.
# This represents peak instantaneous demand for power in that hour.
# source: scraped from the ERCOT website
load_data = read.csv("C:/Users/Haokun Zhang/Desktop/github/ECO395M-1/data/ercot/load_data.csv")
head(load_data)
# Now weather data at hundreds of weather stations
# throughout Texas and the surrounding region
# Note: I've imputed a handful of sporadic missing values
# Source: National Weather Service
temperature_impute = read.csv("C:/Users/Haokun Zhang/Desktop/github/ECO395M-1/data/ercot/temperature_impute.csv", row.names=1)
# Power grid load every hour for 6 1/2 years
# throughout the 8 ERCOT regions of Texas
# units of grid load are megawatts.
# This represents peak instantaneous demand for power in that hour.
# source: scraped from the ERCOT website
load_data = read.csv("C:/Users/Haokun Zhang/Desktop/github/ECO395M-1/data/ercot/ercot/ercot/load_data.csv")
# Now weather data at hundreds of weather stations
# throughout Texas and the surrounding region
# Note: I've imputed a handful of sporadic missing values
# Source: National Weather Service
temperature_impute = read.csv("C:/Users/Haokun Zhang/Desktop/github/ECO395M-1/data/ercot/ercot/ercot/temperature_impute.csv", row.names=1)
station_data = read.csv("C:/Users/Haokun Zhang/Desktop/github/ECO395M-1/data/ercot/ercot/ercot//station_data.csv", row.names=1)
library(tidyverse)
library(usmap)
library(lubridate)
library(randomForest)
library(splines)
library(pdp)
# Note: before loading the data,
# you'll first need to unzip the ercot folder
# (too big for GitHub if not compressed)
# Power grid load every hour for 6 1/2 years
# throughout the 8 ERCOT regions of Texas
# units of grid load are megawatts.
# This represents peak instantaneous demand for power in that hour.
# source: scraped from the ERCOT website
load_data = read.csv("C:/Users/Haokun Zhang/Desktop/github/ECO395M-1/data/ercot/ercot/ercot/load_data.csv")
head(load_data)
# Now weather data at hundreds of weather stations
# throughout Texas and the surrounding region
# Note: I've imputed a handful of sporadic missing values
# Source: National Weather Service
temperature_impute = read.csv("C:/Users/Haokun Zhang/Desktop/github/ECO395M-1/data/ercot/ercot/ercot/temperature_impute.csv", row.names=1)
station_data = read.csv("C:/Users/Haokun Zhang/Desktop/github/ECO395M-1/data/ercot/ercot/ercot//station_data.csv", row.names=1)
# take a peak at the weather station data
head(temperature_impute)
head(station_data)
####
# Data cleaning
####
# some dates have completely missing weather data
# Keep the load data for dates when we have weather data
mysub = which(ymd_hms(load_data$Time) %in% ymd_hms(rownames(temperature_impute)))
load_data = load_data[mysub,]
# De-duplicate the weather data by merging on first match of date in the load data
temp_ind = match(ymd_hms(load_data$Time), ymd_hms(rownames(temperature_impute)))
temperature_impute = temperature_impute[temp_ind,]
# Take the time stamps from the load data
time_stamp = ymd_hms(load_data$Time)
# Verify that the time stamps match row by row across the two data frames
all(time_stamp ==  ymd_hms(rownames(temperature_impute)))
# a lot of these station names are in Mexico or the Gulf
# and we don't have temperature data on them
station_data = subset(station_data, state != 'MX')
# Make a map.
# First, project project the lon, lat coordinates
# to the same coordinate system used by usmap
station_map = station_data %>%
select(lon, lat) %>%
usmap_transform
head(station_map)
# now merge these coordinates station name
station_data = station_data %>% rownames_to_column('station')
station_data = merge(station_data, station_map, by=c('lat', 'lon'))
head(station_data)
# plot the coordinates of the weather stations
plot_usmap(include = c("TX", "LA", "OK", "NM", "AR")) +
geom_point(data=station_data, aes(x=x, y=y))
####
# PCA
####
# Now run PCA on the weather data
pc_weather = prcomp(temperature_impute, rank=5, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_weather$rotation)
install.packages("rgdal")
library(tidyverse)
library(usmap)
library(lubridate)
library(randomForest)
library(splines)
library(pdp)
# Note: before loading the data,
# you'll first need to unzip the ercot folder
# (too big for GitHub if not compressed)
# Power grid load every hour for 6 1/2 years
# throughout the 8 ERCOT regions of Texas
# units of grid load are megawatts.
# This represents peak instantaneous demand for power in that hour.
# source: scraped from the ERCOT website
load_data = read.csv("C:/Users/Haokun Zhang/Desktop/github/ECO395M-1/data/ercot/ercot/ercot/load_data.csv")
head(load_data)
# Now weather data at hundreds of weather stations
# throughout Texas and the surrounding region
# Note: I've imputed a handful of sporadic missing values
# Source: National Weather Service
temperature_impute = read.csv("C:/Users/Haokun Zhang/Desktop/github/ECO395M-1/data/ercot/ercot/ercot/temperature_impute.csv", row.names=1)
station_data = read.csv("C:/Users/Haokun Zhang/Desktop/github/ECO395M-1/data/ercot/ercot/ercot//station_data.csv", row.names=1)
# take a peak at the weather station data
head(temperature_impute)
head(station_data)
####
# Data cleaning
####
# some dates have completely missing weather data
# Keep the load data for dates when we have weather data
mysub = which(ymd_hms(load_data$Time) %in% ymd_hms(rownames(temperature_impute)))
load_data = load_data[mysub,]
# De-duplicate the weather data by merging on first match of date in the load data
temp_ind = match(ymd_hms(load_data$Time), ymd_hms(rownames(temperature_impute)))
temperature_impute = temperature_impute[temp_ind,]
# Take the time stamps from the load data
time_stamp = ymd_hms(load_data$Time)
# Verify that the time stamps match row by row across the two data frames
all(time_stamp ==  ymd_hms(rownames(temperature_impute)))
# a lot of these station names are in Mexico or the Gulf
# and we don't have temperature data on them
station_data = subset(station_data, state != 'MX')
# Make a map.
# First, project project the lon, lat coordinates
# to the same coordinate system used by usmap
station_map = station_data %>%
select(lon, lat) %>%
usmap_transform
head(station_map)
# now merge these coordinates station name
station_data = station_data %>% rownames_to_column('station')
station_data = merge(station_data, station_map, by=c('lat', 'lon'))
head(station_data)
# plot the coordinates of the weather stations
plot_usmap(include = c("TX", "LA", "OK", "NM", "AR")) +
geom_point(data=station_data, aes(x=x, y=y))
####
# PCA
####
# Now run PCA on the weather data
pc_weather = prcomp(temperature_impute, rank=5, scale=TRUE)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_weather$rotation)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_weather)
View(temperature_impute)
View(pc_weather)
# extract the loadings and make the station names a variable
# this will help us with merging
loadings = pc_weather$rotation %>%
as.data.frame %>%
rownames_to_column('station')
# now merge the station location data with the PC loadings
station_data = merge(station_data, loadings, by = 'station')
head(station_data)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_weather)
View(loadings)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_weather$rotation)
# now merge the station location data with the PC loadings
station_data = merge(station_data, loadings, by = 'station')
head(station_data)
# set up the map and the color scale
p0 = plot_usmap(include = c("TX", "LA", "OK", "NM", "AR")) +
scale_color_gradient(low = 'blue', high='red')
# this looks like it contrasts north tx vs elsewhere
# but it is really almost an average temp across texas
# (look at the numbers on the scale)
# the corresponding score is positive when temp is above average across texas
# and negative when temp is below average
p0 + geom_point(data=station_data, aes(x=x, y=y, color=PC1))
scores = pc_weather$x
p1 = pc_weather$x %>%
as.data.frame %>%
rownames_to_column('time') %>%
mutate(time = ymd_hms(time)) %>%
ggplot
# PC score 1 over time
p1 + geom_line(aes(x=time, y=PC1))
# Looking year by year of PC1 versus day of year (1 - 366)
p1 + geom_line(aes(x=yday(time), y=PC1)) + facet_wrap(~year(time))
# PC2 score over time
# Not nearly so periodic
p1 + geom_line(aes(x=time, y=PC2))
p1 + geom_line(aes(x=yday(time), y=PC2)) + facet_wrap(~year(time))
# create a new data frame
# hour of day, day of week, month as predictors
# also use the pc_weather scores
load_combined = data.frame(load_data,
hour = hour(time_stamp),
day = wday(time_stamp),
month = month(time_stamp),
pc_weather$x)
head(station_data)
# notice 5 summary features gets us 95% of the overall variation in 256 original features
# pretty nice compression ratio!
summary(pc_weather)
# these are the linear combinations of station-level data that define the PCs
# each column is a different PC, i.e. a different linear summary of the stations
head(pc_weather$rotation)
# try random forests -- nice "go-to" as a first attempt at block-box predictive modeling
# note: this takes awhile!
forest_coast = randomForest(COAST ~ hour + day + month + PC1 + PC2 + PC3 + PC4 + PC5,
data = load_train, ntree=500)
# create a new data frame
# hour of day, day of week, month as predictors
# also use the pc_weather scores
load_combined = data.frame(load_data,
hour = hour(time_stamp),
day = wday(time_stamp),
month = month(time_stamp),
pc_weather$x)
# split into a training and testing set
train_frac = 0.8
N = nrow(load_combined)
N_train = floor(train_frac*N)
N_test = N - N_train
train_ind = sample.int(N, N_train, replace=FALSE) %>% sort
load_train = load_combined[train_ind,]
load_test = load_combined[-train_ind,]
# try random forests -- nice "go-to" as a first attempt at block-box predictive modeling
# note: this takes awhile!
forest_coast = randomForest(COAST ~ hour + day + month + PC1 + PC2 + PC3 + PC4 + PC5,
data = load_train, ntree=500)
# form predictions and calculate the RMSE on the testing set
yhat_forest_coast = predict(forest_coast, load_test)
mean((yhat_forest_coast - load_test$COAST)^2) %>% sqrt
# form predictions and calculate the RMSE on the testing set
yhat_forest_coast = predict(forest_coast, load_test)
mean((yhat_forest_coast - load_test$COAST)^2) %>% sqrt
# useful to compare to a linear model with each PC expanded in a spline basis
lm_coast = lm(COAST ~ factor(day) + factor(month) + bs(hour, 7) +
bs(PC1, 7) +  bs(PC2, 7) + bs(PC3, 7) + bs(PC4, 7) + bs(PC5, 7),
data=load_train)
yhat_coast_lm2 = predict(lm_coast, load_test)
## read the dataset
df <- read.csv("https://raw.githubusercontent.com/haokunz/Data-mining-project/tree/Haokun/data/internal_control_data.csv",
header = TRUE)
## read the dataset
df <- read.csv("https://raw.githubusercontent.com/haokunz/Data_mining_project/tree/Haokun/data/internal_control_data.csv",
header = TRUE)
## read the dataset
df <- read.csv("https://raw.githubusercontent.com/haokunz/Data_mining_project/main/data/internal_controls_data_1680556746.csv",
header = TRUE)
### Workflow
### 1. 決定問題: Audit fee的定價應為多少
### 2. 準備數據(Completed)
### 3. Data Visualization(EDA - Exploratory Data Analysis)
### 4. Unsupervised learning: Use PCAmix and K-means++ clustering to do the company segmentation? Like 繼鵬他們做的)
### 5. 選擇模型(Supervised learning):
###     Linear regression, knn, Regression tree: random forest, CART, Boosting & Lasso regression
### 6. 訓練模型: Split to Training set & Testing set, 做K-fold Cross-Validation
### 7. 評估模型: k-fold cv, MSE, R-square, F-1 score(the chart of actual and  predicted)
### ( Choose the best performance model，use it to do the prediction，並且將結果圖表化？ Partial dependence plots etc)
### 8. Conclusion: What did we learn- 根據不同的input value, 可以預測audit fee為多少
### Workflow if 學繼鵬
### 1~4 same
### 5. Use lasso regression and do th cv, find the important coefficients(more robust and understandable coefficients)
### 6. Use Random Forest to measure the variable importance(better fit but less interpretability), and use partial plots
### End
## Problem zone
### 1. How to assess the model performance? BC Lasso model 好像不能用MSE去評價？ If not, what can we use to assess the model?
### 2. Model是要用來找出variable的重要性的嗎？ 繼鵬好像是用lasso 找出重要的 independent variable
### 繼鵬他們沒有用模型進行預測，只是針對數據分析結果做出解釋，提出哪些因素對acquire more kisses on dating app有影響
### 如果要學繼鵬他們，就是提出哪些因素對acquire more kisses有影響
# Step 2. Data wrangling & load the needed packages
## Loaded the needed packages
library(tidyverse)
library(ggplot2)
library(rsample)
library(modelr)
library(randomForest)
library(caret)
library(gbm)
library(ggmap)
library(glmnet)
library(rpart) # a powerful ML library that is used for building classification and regression trees
library(gamlr)
library(rpart.plot)
library(data.table)
library(DMwR2)
library(car)
library(factoextra)
library(rfUtilities)
## read the dataset
df <- read.csv("https://raw.githubusercontent.com/haokunz/Data_mining_project/main/data/internal_controls_data_1680556746.csv",
header = TRUE)
## delete copyright and lines of notes
df <- df[-c(nrow(df), nrow(df)-1), ]
## remove records with restated internal control report
duplicated_indexes <- which(df$Restated.Internal.Control.Report == "Yes (1)")
duplicated_companies <- unique(df$Company[duplicated_indexes])
restate_indexes <- which(df$Company %in% duplicated_companies)
remove_index <- setdiff(restate_indexes, duplicated_indexes)
df1 <- df[-remove_index, ]
## remove duplicated records from different auditors working at the same time
multi_auditors <- table(df1$Company)[table(df1$Company) >= 2]
remove_index_2 <- setdiff(which(df1$Company %in% names(multi_auditors)), match(names(multi_auditors), df1$Company))
df2 <- df1[-remove_index_2, ]
## remove rows with missing revenue data
df2 <- df2[df2$Revenue.... != "", ]
## select target columns
df3 <- df2[ ,c("Company", "City", "State.Code", "State.Name", "State.Region",
"Auditor", "Auditor.Key", "Auditor.State.Name",
"Effective.Internal.Controls", "Audit.Fees....", "Non.Audit.Fees....",
"Total.Fees....", "Share.Price", "Market.Cap....", "Revenue....",
"Earnings....", "Book.Value....", "Assets....")]
## change column names to mark the targets
colnames(df3) <- c("company", "city", "state_code", "state_name", "state_region",
"auditor", "auditor_key", "auditor_state_name",
"effective_internal_controls", "audit_fees", "non_audit_fees",
"total_fees", "share_price", "market_cap","revenue",
"earnings", "book_value", "assets")
## convert money amount character into numeric
df3$audit_fees = as.numeric(gsub(",", "", df3$audit_fees))
df3$non_audit_fees = as.numeric(gsub(",", "", df3$non_audit_fees))
df3$total_fees = as.numeric(gsub(",", "", df3$total_fees))
df3$market_cap = as.numeric(gsub(",", "", df3$market_cap))
df3$revenue = as.numeric(gsub(",", "", df3$revenue))
df3$earnings = as.numeric(gsub(",", "", df3$earnings))
df3$book_value = as.numeric(gsub(",", "", df3$book_value))
df3$assets = as.numeric(gsub(",", "", df3$assets))
## add indicator for analysis
df3$big_four_indicator <- ifelse(df3$auditor_key <= 4, 1, 0)
df3$five_category <- ifelse(df3$auditor_key < 5, df3$auditor_key, 5)
df3$audit_percent <- df3$audit_fees / df3$total_fees
## add transformation variables to the data
df3$audit_fees_bc <- predict(BoxCoxTrans(df3$audit_fees), df3$audit_fees)
non_audit_bc <- predict(BoxCoxTrans(df3$non_audit_fees[df3$non_audit_fees!=0]),
df3$non_audit_fees[df3$non_audit_fees!=0])
df3$total_fees_bc <- predict(BoxCoxTrans(df3$total_fees), df3$total_fees)
df3$market_cap_bc <- predict(BoxCoxTrans(df3$market_cap), df3$market_cap)
df3$market_fee_ratio <- log(df3$market_cap/ df3$total_fees)
df3$assets_log <- log(df3$assets)
revenue_0 = jitter(df3$revenue)
df3$revenue_trans <- (revenue_0/abs(revenue_0)) * log(abs(df3$revenue) + 1)
earnings_0 = jitter(df3$earnings)
df3$earnings_trans <- (earnings_0/abs(earnings_0)) * log(abs(df3$earnings) + 1)
## preliminary test on big_four_indicator,change coloumns to factor
# add the big 4 indicator, five category and state region into the dataframe
df3$big_4_factor <- as.factor(df3$big_four_indicator)
df3$five_category_factor <- as.factor(df3$five_category)
df3$state_region <- as.factor(df3$state_region)
# Step 3. Data visualization(EDA - Exploratory Data Analysis)
### Add the explanation for the chart
# basic plots, preliminary exploration #
# plot the number distribution of companies in different regions
company_numbers <- sort(table(df3$state_region[df3$state_region != ""]), decreasing = FALSE, na.last = NA)
par(mar = c(5.1, 6.5, 4.1, 2.1))
barplot(height=company_numbers,
names.arg=c("Canada", "US_NewEng", "US_Southwest", "US_Southeast",
"US_Midwest", "Foreign", "US_MAtlan", "US_West"),
col="#69b3a2", horiz=TRUE, las = 1, main = "Num. of Companies", xlab = "numbers")
par(mar = c(5.1, 4.1, 4.1, 2.1))
# use eight plots to display the effect of transformation on fee related variables
par(mfrow = c(2, 4))
hist(df3$audit_fees, breaks="Scott", main="audit fees", xlab="Audit fees")
hist(df3$audit_fees_bc, main="audit fees (transformed)", xlab="Audit fees")
hist(df3$non_audit_fees, breaks="Scott", main="non audit fees", xlab="Non-audit fees")
hist(non_audit_bc, main="non audit fees (transformed)", xlab="Non-audit fees")
hist(df3$total_fees, breaks="Scott", main="total fees", xlab="Total fees")
hist(df3$total_fees_bc, main="total fees (transformed)", xlab="Total fees")
hist(df3$market_cap, breaks="Scott", main="Market cap", xlab="Market cap")
hist(df3$market_cap_bc, main="Market cap (transformed)", xlab="Market cap")
par(mfrow = c(1, 1))
# use three plots to display the categorical data
par(mfrow = c(1, 3))
barplot(table(df3$five_category_factor), ylab = "Frequency", main="Auditing company distribution")
barplot(table(df3$big_4_factor), yaxt='n', ylab="Frequency", main="Num. big4 vs. other")
axis(side=2, at=seq(0, nrow(df3), 200))
barplot(table(df3$effective_internal_controls), yaxt='n', ylab="Frequency", main="Num. effective internal controls")
axis(side=2, at=seq(0, nrow(df3), 200))
par(mfrow = c(1, 1))
# plot the transformed company market cap, total auditing fees, and effective internal control
sp = ggplot(df3, aes(x=market_cap_bc, y=five_category_factor,
group=effective_internal_controls)) +
geom_point(aes(color=effective_internal_controls), size=0.9,
position=position_dodge2(0.3))
labels = as.vector(outer(rep("Num. of 'No'="), table(df3$effective_internal_controls,
df3$five_category_factor)[1,],
paste, sep=""))
sp + annotate(geom="text", x=rep(27.5, 5), y=seq(0.7, 4.7, 1), label= labels)
# plot the transformed company market cap vs. total auditing fees
ggplot(df3, aes(x=market_cap_bc, y=total_fees_bc, group=five_category_factor)) +
geom_point(aes(color=five_category_factor), size=0.9)
cor(df3$market_cap_bc, df3$total_fees_bc)
# plot the auditing fees
ggplot(df3, aes(x=five_category_factor, y=total_fees_bc)) +
geom_violin(trim=FALSE, fill="gray")+
labs(title="Auditing fees",x="category", y = "total fees")+
geom_boxplot(width=0.3)+
theme_classic()
# Change color by groups
dp <- ggplot(df3, aes(x=five_category_factor, y=total_fees_bc, fill=five_category_factor)) +
geom_violin(trim=FALSE)+
geom_boxplot(width=0.3, fill="white")+
labs(title="Plot of auditing fees",x="category", y = "total fees")
dp + theme_classic()
# Step 4. Unsupervised learning: Use PCAmix and K-means++ clustering to do the company segmentation
# Step 5. Modeling: Linear regression, knn, Regression tree: random forest, CART, Boosting
### Base model: Linear regression(Bid model)
## Syntax: lm3 = lm(price ~ (. - pctCollege - sewer - waterfront - landValue - newConstruction)^2, data=saratoga_train)
### Model 2: KNN
## Syntax: KNN with K = 70
## knn100 = knnreg(COAST ~ KHOU, data=loadhou_train, k=100)
### modelr::rmse(knn100, loadhou_test)
## predict(knn100, loadhou_test)
### Model 3 to 5 are belong to regression trees
### Syntax in Exercise3 Q2
### Model 3: Random Forest model
## Syntax: rforest_dengue = randomForest(total_cases ~ .,data = dengue_training, importance=TRUE)
## Performance: Use rmse. Syntax: rmse(gbm_dengue, dengue_testing)
### Model 4: CART model
## Syntax: cart_dengue = rpart(total_cases ~ . , data = dengue_training, control = rpart.control(cp = 0.002, minsplit=20))
## Split only if we have at least 20 obs in a node,
## and the split improves the fit by a factor of 0.002 aka 0.2%
## Performance: Use rmse. Syntax: rmse(gbm_dengue, dengue_testing)
### Model 5: Gradient-boosted model
## in the "capmetro.R"
## Syntax: gbm_dengue = gbm(total_cases ~ ., data = dengue_training, interaction.depth=4, n.trees=500, shrinkage=.05)
## Performance: Use rmse. Syntax: rmse(gbm_dengue, dengue_testing)
### Model 6: Lasso regression
### Use lasso to find the important variables?
## 好像不能用MSE out of sample去評價，要用AIC？
## Syntax in homework exercise 2
## Syntax for Lasso:
### lasso_selected = glm(children ~ (.-arrival_date-deposit_type) + hotel:reserved_room_type+ meal:is_repeated_guest+ adults:previous_bookings_not_canceled+ meal:previous_bookings_not_canceled+ market_segment:customer_type+is_repeated_guest:assigned_room_type+ assigned_room_type:required_car_parking_spaces, data = hotels_dev_train, family = "binomial")
# Step 6. 訓練模型: Split to Training set & Testing set, 做K-fold Cross-Validation
## Split to Training set & Testing set
### Syntax:
### hotels_dev_split = initial_split(hotels_dev, prop = 0.7)
### hotels_dev_train = training(hotels_dev_split)
### hotels_dev_test = testing(hotels_dev_split)
### Step 7. 評價模型: K-fold Cross-validation
## K-fold Cross-validation: (k-1) is training set, 1 is testing set
## 從沒當過testing set的 dataset中挑一個來做testing set, 剛剛做過testing set 的那份則加回去做training set
## repeat the step until every set 都當過testing set ## 會執行k次, 得到k個 validation error
## Average the k validation error, then we can know which model is better
### Syntax:
#### set.seed(123)
#### k <- 5
#### cart_cv <- rpart.control(cp = 0.01)
#### rf_cv <- list(mtry = sqrt(ncol(dengue_training)), replace = TRUE)
#### gb_cv <- list(n.trees = 1000, interaction.depth = 4, shrinkage = 0.01, cv.folds = k)
#### cart_cv_results <- rpart(total_cases ~ ., data = dengue_training, control = cart_cv)
#### rf_cv_results <- randomForest(total_cases ~ ., data = dengue_training, mtry = rf_cv$mtry, replace = rf_cv$replace)
#### gb_cv_results <- gbm(total_cases ~ ., data = dengue_training, n.trees = gb_cv$n.trees, interaction.depth = gb_cv$interaction.depth, shrinkage = gb_cv$shrinkage, cv.folds = gb_cv$cv.folds, verbose = FALSE)
#### Evaluate the performance of each model
#### cart_performance <- predict(cart_cv_results, newdata = dengue_testing)
#### rf_performance <- predict(rf_cv_results, newdata = dengue_testing)
#### gb_performance <- predict(gb_cv_results, newdata = dengue_testing, n.trees = gb_cv$n.trees)
#### Compare the performance of each model by measuring MSE
#### cart_accuracy <- mean((cart_performance - dengue_testing$total_cases)^2)
#### rf_accuracy <- mean((rf_performance - dengue_testing$total_cases)^2)
#### gb_accuracy <- mean((gb_performance - dengue_testing$total_cases)^2)
####  print out the MSE of each model to console, the lower the better the model is
#### cat("CART accuracy:", cart_accuracy, "\n")
#### cat("Random Forest accuracy:", rf_accuracy, "\n")
#### cat("Gradient Boosting accuracy:", gb_accuracy, "\n")
### Syntax for k-fold cv ends
library(LICORS)
install.packages("ILS")
install.packages("LICORS")
